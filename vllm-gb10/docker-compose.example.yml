services:
  vllm:
    image: YOUR_DOCKERHUB_USERNAME/vllm-gb10:latest
    container_name: vllm-inference
    
    # Model and server configuration
    command:
      - --model
      - zai-org/GLM-4.7-Flash  # Change to your model
      - --host
      - 0.0.0.0
      - --port
      - 8000
      - --trust-remote-code
      - --gpu-memory-utilization
      - 0.85  # Adjust based on your GPU memory
      - --max-model-len
      - 4096  # Increase for longer contexts
      - --dtype
      - bfloat16  # or auto, float16
      - --tensor-parallel-size
      - 1  # Increase for multi-GPU
      # Optional: For GLM-4.7-Flash specific features
      # - --tool-call-parser
      # - glm47
      # - --reasoning-parser
      # - glm45
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=  # Set in .env file
      - HF_HOME=/data/cache/huggingface
      - CUDA_VISIBLE_DEVICES=0  # Specify GPU(s)
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
    
    # Volume mounts
    volumes:
      # Model storage (optional, uses HuggingFace cache by default)
      - ./models:/data/models:rw
      # HuggingFace cache (recommended to persist downloads)
      - ~/.cache/huggingface:/data/cache/huggingface:rw
      # Logs (optional)
      - ./logs:/workspace/logs:rw
    
    # Port mapping
    ports:
      - 8000:8000  # OpenAI-compatible API
    
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # or specify count: 1, count: 2, etc.
              capabilities: [gpu]
    
    # Restart policy
    restart: unless-stopped
    
    # Health check
    healthcheck:
      test: [CMD, curl, -f, http://localhost:8000/health]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # 5 minutes for model loading
