# Local Agentic Software Development Platform 

This repository contains documentation and configuration for deploying a **production dual-GPU AI infrastructure** with self-hosted Open Source LLMs on NVIDIA Blackwell hardware (GB10/Grace Blackwell).

All AI inference runs locally on your own GPU hardware, giving you full control, privacy, and no API costs.

## ğŸ¯ Current Deployment

**Two production systems serving complementary roles:**

- **Pegasus** - [GPT-OSS-120B](systems/pegasus/) (117B params) - Architect & Analyst
- **Stella** - [GLM-4.7-Flash-NVFP4](systems/stella/) (30B MoE) - Fast Inference

ğŸ“Š **[View Current Status â†’](STATUS.md)** | ğŸ“– **[View Changelog â†’](CHANGELOG.md)**

## ğŸš€ Production Systems

### Pegasus - GPT-OSS-120B
**Status**: âœ… Operational | **API**: http://pegasus.home.arpa:8000

- **Model**: OpenAI GPT-OSS-120B (117B params, MXFP4 quantized)
- **Performance**: 34 tokens/sec sustained
- **Context**: 131,072 tokens
- **Role**: Long-context analysis, architecture design, code review
- **Features**: OpenAI-compatible tool calling

ğŸ“– **[Documentation â†’](systems/pegasus/)** | ğŸš€ **[Quick Start â†’](systems/pegasus/QUICKSTART.md)**

### Stella - GLM-4.7-Flash-NVFP4
**Status**: ğŸ”„ Deploying | **API**: http://stella.home.arpa:8000

- **Model**: GadflyII/GLM-4.7-Flash-NVFP4 (30B MoE, NVFP4 quantized)
- **Target**: 50+ tokens/sec
- **Context**: Up to 202,752 tokens
- **Role**: Fast interactive chat, quick queries
- **Special**: Blackwell-optimized mixed precision quantization

ğŸ“– **[Documentation â†’](systems/stella/)**

## ğŸ“¦ Projects

### [vLLM for NVIDIA GB10](vllm-gb10/)

Self-contained vLLM project with native GB10/Blackwell support. This is the foundation for both Pegasus and Stella deployments.

**Features:**
- Native GB10/Blackwell support (SM 12.1)
- Multi-architecture CUDA kernels
- OpenAI-compatible API
- Docker-based deployment
- Comprehensive documentation

ğŸ“– **[vLLM GB10 Project â†’](vllm-gb10/)**

## ğŸ—ï¸ Architecture

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Frontend / Clients        â”‚
                 â”‚   - OpenCode / Cursor       â”‚
                 â”‚   - API clients             â”‚
                 â”‚   - SSH access              â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       OpenAI API & SSH        â”‚
                â”‚                               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     Pegasus      â”‚           â”‚      Stella      â”‚
       â”‚  ASUS Ascend     â”‚           â”‚  ASUS Ascent     â”‚
       â”‚  GB10 (128GB)    â”‚           â”‚  GX10 GB (128GB) â”‚
       â”‚                  â”‚           â”‚  Grace Blackwell â”‚
       â”‚  GPT-OSS-120B    â”‚           â”‚  GLM-4.7-NVFP4   â”‚
       â”‚  117B params     â”‚           â”‚  30B MoE         â”‚
       â”‚  34 tok/s        â”‚           â”‚  50+ tok/s       â”‚
       â”‚  :8000           â”‚           â”‚  :8000           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                               â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   NFS Model Storage  â”‚
                    â”‚  flashstore.arpa     â”‚
                    â”‚  9.1TB RAID5         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Documentation Structure

```
â”œâ”€â”€ STATUS.md                    # Current deployment status
â”œâ”€â”€ CHANGELOG.md                 # Project timeline and changes
â”œâ”€â”€ systems/                     # Per-system documentation
â”‚   â”œâ”€â”€ pegasus/                # GPT-OSS-120B deployment
â”‚   â””â”€â”€ stella/                 # GLM-4.7-Flash-NVFP4 deployment
â”œâ”€â”€ docs/                       # Supporting documentation
â”‚   â”œâ”€â”€ deployment/             # Deployment guides
â”‚   â”œâ”€â”€ network/                # Network configuration
â”‚   â”œâ”€â”€ archive/                # Historical documents
â”‚   â””â”€â”€ research/               # Future projects
â”œâ”€â”€ vllm-gb10/                  # vLLM GB10 project (self-contained)
â””â”€â”€ scripts/                    # Utility scripts
```

### ğŸ“– Key Documents

**System Documentation**:
- [Pegasus (GPT-OSS-120B)](systems/pegasus/) - Production Architect & Analyst system
- [Stella (GLM-4.7-Flash-NVFP4)](systems/stella/) - Fast inference system

**Status & History**:
- [STATUS.md](STATUS.md) - Current deployment status
- [CHANGELOG.md](CHANGELOG.md) - Project timeline

**Deployment**:
- [vLLM GB10 Setup Guide](vllm-gb10/SETUP.md) - Complete GB10 build guide

## ğŸš€ Quick Start

### Check System Status
```bash
# View current deployment status
cat STATUS.md

# Check Pegasus
curl http://pegasus.home.arpa:8000/health

# Check Stella (when deployed)
curl http://stella.home.arpa:8000/health
```

### Using the APIs

See individual system documentation:
- **Pegasus**: [systems/pegasus/QUICKSTART.md](systems/pegasus/QUICKSTART.md)
- **Stella**: [systems/stella/QUICKSTART.md](systems/stella/QUICKSTART.md) (coming soon)

## Contributing

Contributions welcome! See [vllm-gb10/CONTRIBUTING.md](vllm-gb10/CONTRIBUTING.md).

## License

MIT License - see [LICENSE](LICENSE) for details.

Individual components may have their own licenses:
- vLLM: Apache 2.0
- Docker images: Based on official vLLM images
