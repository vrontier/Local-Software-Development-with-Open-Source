# Storage-Index-Search System (SISS) — Design Specification v2

> v1 generated by GPT-OSS-120B on Pegasus — 2026-02-12
> v2 revised by Claude Opus 4.6 — 2026-02-12
>
> This revision corrects the MCP protocol design (JSON-RPC 2.0, not gRPC),
> introduces a tiered deployment model (Docker Compose MVP → K3s → K8s),
> eliminates unnecessary blob duplication, integrates local LLM infrastructure,
> and compresses the roadmap from 38 weeks to a usable MVP in 6-8 weeks.

---

## Changelog from v1

| Area | v1 Issue | v2 Fix |
|------|----------|--------|
| **MCP Protocol** | Defined as gRPC + protobuf | Corrected to JSON-RPC 2.0 over stdio/SSE per Anthropic's MCP spec |
| **Architecture** | Jumped straight to K8s + Istio | Tiered: Docker Compose MVP → K3s → K8s |
| **Blob Storage** | Copied all files into MinIO (doubles storage) | NAS remains source of truth; MinIO only at scale tier |
| **Search Engine** | OpenSearch only (heavy) | Meilisearch for MVP, OpenSearch at scale |
| **Language Choice** | Go + Java (two runtimes) | Rust for core services (single binary, lower memory) |
| **LLM Integration** | None | Leverages existing Pegasus/Stella for embeddings and summarization |
| **Synology** | Treated as dumb NFS mount | Leverages DSM APIs, Synology Docker package, native features |
| **Roadmap** | 38 weeks (~9 months) | MVP in 6-8 weeks, production in 16-20 weeks |
| **Deduplication** | None | Content-addressable via SHA-256 |
| **Structured Queries** | Generic field filters | XPath and JSONPath native query support |
| **Real-time** | None | WebSocket/SSE for live index update notifications |

---

## 1. High-Level Architecture

### 1.1 Tiered Deployment Model

The system is designed to run at three scale tiers. Start at Tier 1 and promote when capacity demands it.

```
┌─────────────────────────────────────────────────────────────────────┐
│ TIER 1: MVP (Docker Compose on Synology or single Linux host)      │
│                                                                     │
│  ┌──────────┐  ┌───────────┐  ┌────────────┐  ┌──────────────────┐ │
│  │ Caddy    │  │ SISS Core │  │ Meilisearch│  │ PostgreSQL       │ │
│  │ (reverse │──│ (Rust)    │──│ (search)   │  │ (metadata + FTS) │ │
│  │  proxy)  │  │           │──│            │  │                  │ │
│  └──────────┘  │ REST API  │  └────────────┘  └──────────────────┘ │
│                │ MCP Server│                                        │
│                │ Ingestor  │───── /volume1/data (NAS direct mount)  │
│                │ Parser    │                                        │
│                └───────────┘                                        │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│ TIER 2: Multi-node (K3s, 2-5 nodes)                                │
│                                                                     │
│  Adds: Redis cache, NATS event bus, horizontal API replicas,       │
│        worker pool for async ingestion, dedicated search nodes      │
│  NAS: Still primary via NFS mount on all nodes                     │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│ TIER 3: Production (K8s cluster)                                    │
│                                                                     │
│  Adds: OpenSearch (replaces Meilisearch), MinIO (blob replication),│
│        Keycloak (OIDC), Istio (mTLS), Patroni (HA Postgres),      │
│        Prometheus/Grafana/Jaeger observability stack                │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.2 Detailed Tier 1 Architecture

```
                        Clients
                 ┌────────┼────────┐
                 │        │        │
              Web UI   CLI/SDK   AI Agents
                 │        │        │
                 └────────┼────────┘
                          │
                    ┌─────┴──────┐
                    │   Caddy    │  :443 (HTTPS + auto-TLS)
                    │   reverse  │  :80  (redirect)
                    │   proxy    │
                    └─────┬──────┘
                          │
              ┌───────────┼───────────┐
              │                       │
     ┌────────┴────────┐    ┌────────┴────────┐
     │  REST API       │    │  MCP Server     │
     │  :8080          │    │  :3000 (SSE)    │
     │  (Axum/Rust)    │    │  stdio (local)  │
     └────────┬────────┘    └────────┬────────┘
              │                       │
              └───────────┬───────────┘
                          │
              ┌───────────┼───────────┐
              │           │           │
     ┌────────┴──┐  ┌────┴─────┐  ┌──┴──────────┐
     │ PostgreSQL│  │Meilisearch│  │ NAS (NFS)   │
     │ :5432     │  │ :7700    │  │ /volume1/    │
     │ metadata  │  │ full-text│  │ data/        │
     │ + schemas │  │ + facets │  │ (source of   │
     └───────────┘  └──────────┘  │  truth)      │
                                  └──────────────┘
                          │
              ┌───────────┼───────────┐
              │                       │
     ┌────────┴────────┐    ┌────────┴────────┐
     │  Pegasus        │    │  Stella         │
     │  (GPT-OSS-120B) │    │  (Qwen3-8B)    │
     │  Summarization  │    │  Embeddings     │
     │  Schema analysis│    │  Classification │
     └─────────────────┘    └─────────────────┘
```

**Key design decisions:**
- **NAS is the source of truth** — files are never duplicated into an object store at Tier 1. The index points to NAS paths. This avoids doubling your storage.
- **Single Rust binary** — the API, MCP server, ingestor, and parser are compiled into one binary with feature flags. No Java/Tika dependency.
- **Meilisearch over OpenSearch** — 10x less memory, instant setup, excellent faceting and typo tolerance. Swap to OpenSearch at Tier 3 when you need sharding across nodes.
- **Caddy over NGINX** — automatic HTTPS, simpler config, built-in reverse proxy. Swap to NGINX/Kong at Tier 3.

---

## 2. Component Breakdown & Technology Choices

### 2.1 Tier 1 (MVP)

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Core Service** | **Rust** (Axum web framework) | Single binary, ~10 MB memory, safe concurrency, fast XML/JSON parsing via `serde`, `quick-xml`, `serde_json`. No GC pauses. |
| **Search Engine** | **Meilisearch** | Sub-50ms search, typo tolerance, faceted filtering, ~200 MB RAM for 1M docs. RESTful API. Apache-2.0. |
| **Metadata DB** | **PostgreSQL 17** | JSONB for flexible metadata, GIN indexes, `pg_trgm` for fuzzy text matching, `pgvector` for embeddings. |
| **Reverse Proxy** | **Caddy** | Auto-TLS (Let's Encrypt or self-signed), reverse proxy, rate limiting. Zero-config HTTPS. |
| **File Storage** | **Synology NAS** (NFS/SMB direct) | No duplication. Files served directly from NAS via presigned internal URLs or streamed through the API. |
| **MCP Server** | **Rust** (built into core) | JSON-RPC 2.0 over stdio (for local agents) and SSE (for remote agents). Uses `rmcp` crate. |
| **Embeddings** | **Stella** (Qwen3-8B, local) | Generate document embeddings via local inference. 2,236 tok/s prompt processing handles batch embedding efficiently. |
| **Summarization** | **Pegasus** (GPT-OSS-120B, local) | Generate file summaries, infer schema descriptions, classify document types. 45+ tok/s generation. |
| **Container Runtime** | **Docker Compose** | Single `docker-compose.yml`. Can run on Synology DSM Docker package or any Linux host. |

### 2.2 Tier 2 Additions

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Event Bus** | **NATS** | Lighter than Kafka (single binary, 10 MB), JetStream for persistence, pub/sub for index update notifications. |
| **Cache** | **Redis** (or **Valkey**) | Query result caching, rate limiting counters, session store. |
| **Task Queue** | **NATS JetStream** | Durable work queues for async ingestion, re-indexing jobs. No need for a separate RabbitMQ/Celery. |
| **Real-time** | **WebSocket + SSE** | Push notifications when files are indexed, deleted, or modified. |

### 2.3 Tier 3 Additions

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Search Engine** | **OpenSearch** (replaces Meilisearch) | Sharding, cross-node replication, native k-NN for vector search at scale. |
| **Object Store** | **MinIO** (S3-compatible) | Blob replication across nodes for HA. NAS becomes ingest source; MinIO becomes serving layer. |
| **Auth** | **Keycloak** + **OPA** | OIDC/OAuth2 identity provider, fine-grained ABAC policies. |
| **Service Mesh** | **Istio** or **Linkerd** | mTLS between services, traffic shaping, observability. |
| **Orchestration** | **Kubernetes** | HPA, rolling upgrades, PodDisruptionBudgets. |
| **Observability** | **Prometheus + Grafana + Jaeger** | Metrics, dashboards, distributed tracing. |
| **HA Postgres** | **Patroni** | Automatic failover, streaming replication, read replicas. |

---

## 3. Data Flow

### 3.1 Ingestion Pipeline

```
NAS File System
     │
     ├── inotify watcher (real-time) ──┐
     │                                  │
     └── periodic scan (cron, hourly) ──┤
                                        │
                                        ▼
                              ┌─────────────────┐
                              │  File Discovery  │
                              │  & Dedup Check   │
                              │  (SHA-256 hash)  │
                              └────────┬─────────┘
                                       │
                          ┌────────────┼────────────┐
                          │ known hash │            │ new/changed
                          │ (skip)     │            │
                          ▼            │            ▼
                        [done]         │   ┌────────────────┐
                                       │   │ Content Type   │
                                       │   │ Detection      │
                                       │   │ (magic bytes   │
                                       │   │  + extension)  │
                                       │   └───────┬────────┘
                                       │           │
                              ┌────────┴───────────┼──────────────┐
                              │                    │              │
                              ▼                    ▼              ▼
                      ┌──────────────┐   ┌──────────────┐  ┌──────────────┐
                      │ ASCII Parser │   │ JSON Parser  │  │ XML Parser   │
                      │              │   │ (serde_json) │  │ (quick-xml)  │
                      │ line count,  │   │ field types, │  │ XPath tree,  │
                      │ encoding,    │   │ nesting,     │  │ namespaces,  │
                      │ structure    │   │ arrays,      │  │ attributes,  │
                      │ detection    │   │ JSON Schema  │  │ XSD-like     │
                      └──────┬───────┘   └──────┬───────┘  └──────┬───────┘
                             │                  │                 │
                             └──────────┬───────┘─────────────────┘
                                        │
                                        ▼
                              ┌─────────────────┐
                              │ Schema Registry  │
                              │ (PostgreSQL)     │
                              │                  │
                              │ Deduplicate      │
                              │ schemas across   │
                              │ similar files    │
                              └────────┬─────────┘
                                       │
                          ┌────────────┼────────────┐
                          │            │            │
                          ▼            ▼            ▼
                   ┌────────────┐ ┌─────────┐ ┌──────────────┐
                   │ Meilisearch│ │PostgreSQL│ │ LLM Pipeline │
                   │ full-text  │ │ metadata │ │ (optional)   │
                   │ + facets   │ │ + JSONB  │ │              │
                   │ + filters  │ │ + vector │ │ Stella:      │
                   └────────────┘ └─────────┘ │  embeddings  │
                                              │ Pegasus:     │
                                              │  summaries   │
                                              └──────────────┘
```

### 3.2 Deduplication Strategy

Files are content-addressed by SHA-256. On discovery:

1. Compute SHA-256 of file contents
2. Check `files` table — if hash exists AND NAS path matches, skip (unchanged)
3. If hash exists but path differs, create a new `file_paths` entry pointing to the same `file_id` (same content, different location = symlink-like dedup)
4. If hash is new, proceed with full parse + index pipeline
5. If hash changed for an existing path, mark old version and re-index

This avoids re-processing identical files that appear in multiple directories (common with XML config files and JSON schemas).

### 3.3 LLM-Enhanced Indexing (Optional Pipeline)

When enabled, each newly indexed file is also processed through the local LLM infrastructure:

```
New file indexed
       │
       ├──► Stella (Qwen3-8B): Generate 768-dim embedding
       │    POST http://stella.home.arpa:8000/v1/embeddings
       │    → stored in pgvector for semantic search
       │
       └──► Pegasus (GPT-OSS-120B): Generate summary + classification
            POST http://pegasus.home.arpa:8000/v1/chat/completions
            Prompt: "Summarize this {mime} file. Classify its purpose.
                     Extract key entities and relationships."
            → stored in files.ai_summary, files.ai_tags
```

**Rate limiting**: Batch embeddings through Stella at ~2,200 tok/s prompt processing. Queue large-file summaries through Pegasus (one at a time given 120B model size). Prioritize recently accessed files.

### 3.4 Search & Retrieval Flow

```
Client Request
       │
       ▼
  ┌──────────┐     ┌─────────────────────────────────────┐
  │ Parse    │     │ Query Router                         │
  │ query    │────►│                                      │
  │ intent   │     │  "full-text search" → Meilisearch    │
  └──────────┘     │  "JSONPath query"   → PostgreSQL     │
                   │  "XPath query"      → PostgreSQL     │
                   │  "semantic search"  → pgvector        │
                   │  "hybrid"           → merge + re-rank │
                   └──────────────┬──────────────────────┘
                                  │
                                  ▼
                        ┌─────────────────┐
                        │ Result Merger   │
                        │ & Re-ranker     │
                        │                 │
                        │ Reciprocal Rank │
                        │ Fusion (RRF)    │
                        └────────┬────────┘
                                 │
                                 ▼
                        ┌─────────────────┐
                        │ Response with:  │
                        │ - file metadata │
                        │ - highlights    │
                        │ - snippets      │
                        │ - NAS path      │
                        │ - download URL  │
                        └─────────────────┘
```

---

## 4. API Specification

### 4.1 REST JSON API (OpenAPI 3.1)

**Base URL**: `https://siss.home.arpa/api/v1`

#### File Operations

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/files` | List files with filtering, pagination, faceting |
| `GET` | `/files/{id}` | Get file metadata, schema, AI summary |
| `GET` | `/files/{id}/content` | Stream raw file content |
| `GET` | `/files/{id}/download` | Download with `Content-Disposition: attachment` |
| `GET` | `/files/{id}/schema` | Get inferred schema (JSON Schema or XSD-like) |
| `GET` | `/files/{id}/versions` | List all versions of a file (by path) |
| `POST` | `/files` | Upload single file (multipart) |
| `POST` | `/files/batch` | Upload multiple files (multipart or JSON manifest) |
| `PATCH` | `/files/{id}` | Update mutable metadata (tags, description) |
| `DELETE` | `/files/{id}` | Soft-delete (removes from index, keeps on NAS) |

#### Search Operations

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/search` | Simple full-text search (`?q=term`) |
| `POST` | `/search` | Advanced search with structured query DSL |
| `POST` | `/search/semantic` | Vector similarity search using embeddings |
| `POST` | `/search/hybrid` | Combined full-text + semantic with RRF re-ranking |
| `POST` | `/search/xpath` | XPath query against XML documents |
| `POST` | `/search/jsonpath` | JSONPath query against JSON documents |

#### Schema & Analytics

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/schemas` | List all discovered schemas |
| `GET` | `/schemas/{id}` | Get schema definition |
| `GET` | `/schemas/{id}/files` | List files matching a schema |
| `GET` | `/stats` | Index statistics (file count, size, types, health) |
| `GET` | `/stats/facets/{field}` | Top values for a given field |

#### Ingestion Control

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/ingest/scan` | Trigger a full NAS re-scan |
| `GET` | `/ingest/status` | Current ingestion queue depth, processing rate |
| `POST` | `/ingest/reindex/{id}` | Force re-index of a specific file |

#### Advanced Search DSL

```json
POST /api/v1/search
{
  "query": {
    "text": "configuration database",
    "filters": {
      "mime": ["application/json", "text/xml"],
      "tags": ["production"],
      "size_range": {"min": 1024, "max": 10485760},
      "indexed_after": "2026-01-01T00:00:00Z"
    },
    "json_path": "$.database.connections[?(@.host)]",
    "xpath": "//server[@environment='production']",
    "semantic": {
      "text": "database connection configuration",
      "weight": 0.3
    }
  },
  "sort": [{"field": "relevance", "order": "desc"}],
  "facets": ["mime", "tags", "schema_id"],
  "page": 1,
  "size": 20,
  "highlight": true
}
```

**Response:**

```json
{
  "total": 142,
  "page": 1,
  "size": 20,
  "took_ms": 23,
  "items": [
    {
      "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
      "name": "db-config.json",
      "path": "/volume1/data/configs/db-config.json",
      "mime": "application/json",
      "size": 2048,
      "sha256": "a1b2c3...",
      "schema_id": "sch-001",
      "tags": ["production", "database"],
      "ai_summary": "PostgreSQL connection pool configuration for the production cluster with read replicas.",
      "indexed_at": "2026-02-10T14:30:00Z",
      "highlights": {
        "content": "...max_<em>connections</em>: 100, <em>database</em>: prod_main..."
      },
      "score": 0.94
    }
  ],
  "facets": {
    "mime": [
      {"value": "application/json", "count": 89},
      {"value": "text/xml", "count": 53}
    ],
    "tags": [
      {"value": "production", "count": 120},
      {"value": "staging", "count": 22}
    ]
  }
}
```

**Error Model:**

```json
{
  "error": {
    "code": "INVALID_QUERY",
    "message": "Invalid JSONPath expression at position 12",
    "details": {
      "expression": "$.database[invalid",
      "position": 12,
      "expected": "bracket close or filter expression"
    }
  }
}
```

### 4.2 MCP Server (Model Context Protocol)

**Protocol**: JSON-RPC 2.0 over **stdio** (local agents) and **SSE** (remote agents via `https://siss.home.arpa/mcp`)

The MCP server exposes SISS capabilities as tools and resources that AI agents (Claude Code, Claude Desktop, OpenCode, LangChain, etc.) can discover and invoke natively.

#### MCP Server Manifest

```json
{
  "name": "siss",
  "version": "1.0.0",
  "description": "Storage-Index-Search System — search, retrieve, and manage indexed files on NAS storage",
  "capabilities": {
    "tools": true,
    "resources": true,
    "prompts": true
  }
}
```

#### MCP Tools

```json
[
  {
    "name": "search_files",
    "description": "Full-text search across all indexed ASCII, XML, and JSON files. Supports filters by MIME type, tags, date range, and file size. Returns matching files with relevance-ranked highlights.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "query": {
          "type": "string",
          "description": "Free-text search query"
        },
        "mime_filter": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Filter by MIME types, e.g. ['application/json', 'text/xml']"
        },
        "tags": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Filter by tags"
        },
        "max_results": {
          "type": "integer",
          "default": 10,
          "description": "Maximum results to return (1-100)"
        }
      },
      "required": ["query"]
    }
  },
  {
    "name": "search_semantic",
    "description": "Semantic similarity search using vector embeddings. Finds files conceptually related to the query even without exact keyword matches.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "query": {
          "type": "string",
          "description": "Natural language description of what you're looking for"
        },
        "max_results": {
          "type": "integer",
          "default": 10
        }
      },
      "required": ["query"]
    }
  },
  {
    "name": "query_jsonpath",
    "description": "Execute a JSONPath query across all indexed JSON files. Returns matching values with file context.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "expression": {
          "type": "string",
          "description": "JSONPath expression, e.g. '$.servers[*].hostname'"
        },
        "file_filter": {
          "type": "string",
          "description": "Optional glob pattern to limit which files to query, e.g. 'configs/*.json'"
        }
      },
      "required": ["expression"]
    }
  },
  {
    "name": "query_xpath",
    "description": "Execute an XPath query across all indexed XML files. Returns matching nodes with file context.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "expression": {
          "type": "string",
          "description": "XPath expression, e.g. '//server[@env=\"prod\"]/@hostname'"
        },
        "file_filter": {
          "type": "string",
          "description": "Optional glob pattern to limit which files to query"
        }
      },
      "required": ["expression"]
    }
  },
  {
    "name": "get_file",
    "description": "Retrieve the full content of a file by its ID or NAS path. For large files, returns a truncated preview with a download URL.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "file_id": {
          "type": "string",
          "description": "File UUID"
        },
        "path": {
          "type": "string",
          "description": "NAS path (alternative to file_id)"
        },
        "max_bytes": {
          "type": "integer",
          "default": 102400,
          "description": "Maximum content bytes to return (default 100KB)"
        }
      }
    }
  },
  {
    "name": "get_file_schema",
    "description": "Get the inferred schema (JSON Schema or element tree) for a structured file.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "file_id": {"type": "string"}
      },
      "required": ["file_id"]
    }
  },
  {
    "name": "upload_file",
    "description": "Upload a new file to the NAS and trigger indexing. Content can be provided inline (base64) or as a reference to a URL.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "name": {
          "type": "string",
          "description": "Filename including extension"
        },
        "path": {
          "type": "string",
          "description": "Target directory on NAS, e.g. '/volume1/data/uploads/'"
        },
        "content_base64": {
          "type": "string",
          "description": "File content encoded as base64"
        },
        "tags": {
          "type": "array",
          "items": {"type": "string"}
        }
      },
      "required": ["name", "path", "content_base64"]
    }
  },
  {
    "name": "upload_batch",
    "description": "Upload multiple files in a single operation. Each file specifies name, path, and content.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "files": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {"type": "string"},
              "path": {"type": "string"},
              "content_base64": {"type": "string"},
              "tags": {"type": "array", "items": {"type": "string"}}
            },
            "required": ["name", "path", "content_base64"]
          }
        }
      },
      "required": ["files"]
    }
  },
  {
    "name": "list_schemas",
    "description": "List all discovered document schemas with file counts. Useful for understanding the structure of the data corpus.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "mime_filter": {"type": "string"}
      }
    }
  },
  {
    "name": "get_stats",
    "description": "Get index statistics: total files, storage used, file type distribution, indexing queue status.",
    "inputSchema": {
      "type": "object",
      "properties": {}
    }
  },
  {
    "name": "trigger_rescan",
    "description": "Trigger a full re-scan of the NAS to discover new or changed files.",
    "inputSchema": {
      "type": "object",
      "properties": {
        "path": {
          "type": "string",
          "description": "Optional: limit scan to a specific directory"
        }
      }
    }
  }
]
```

#### MCP Resources

```json
[
  {
    "uri": "siss://stats",
    "name": "Index Statistics",
    "description": "Current index health: file counts by type, storage usage, queue depth",
    "mimeType": "application/json"
  },
  {
    "uri": "siss://schemas",
    "name": "Schema Registry",
    "description": "All discovered document schemas",
    "mimeType": "application/json"
  },
  {
    "uri": "siss://file/{file_id}",
    "name": "File Content",
    "description": "Raw content of a specific indexed file",
    "mimeType": "application/octet-stream"
  },
  {
    "uri": "siss://recent",
    "name": "Recently Indexed",
    "description": "Last 50 files added to the index",
    "mimeType": "application/json"
  }
]
```

#### MCP Prompts

```json
[
  {
    "name": "explore_data",
    "description": "Guided exploration of the indexed data corpus. Starts with stats, suggests searches based on discovered schemas.",
    "arguments": [
      {
        "name": "focus",
        "description": "Optional area of interest, e.g. 'XML configs' or 'JSON API responses'",
        "required": false
      }
    ]
  },
  {
    "name": "find_related",
    "description": "Given a file ID, find semantically and structurally related files.",
    "arguments": [
      {
        "name": "file_id",
        "description": "The source file to find relatives of",
        "required": true
      }
    ]
  }
]
```

#### MCP Configuration (claude_desktop_config.json)

```json
{
  "mcpServers": {
    "siss": {
      "command": "siss-mcp",
      "args": ["--mode", "stdio"],
      "env": {
        "SISS_API_URL": "https://siss.home.arpa/api/v1",
        "SISS_API_KEY": "your-api-key"
      }
    }
  }
}
```

Or for remote SSE transport:

```json
{
  "mcpServers": {
    "siss": {
      "url": "https://siss.home.arpa/mcp",
      "headers": {
        "Authorization": "Bearer your-api-key"
      }
    }
  }
}
```

---

## 5. Storage Layout & Schema Design

### 5.1 NAS Directory Structure (Source of Truth)

```
/volume1/data/                    ← SISS-watched root (configurable)
 ├── configs/                     ← XML/JSON config files
 ├── logs/                        ← ASCII log files
 ├── exports/                     ← data exports
 ├── uploads/                     ← files uploaded via SISS API
 └── .siss/                       ← SISS metadata (on NAS)
     ├── manifest.json            ← scan state, last-modified timestamps
     └── thumbnails/              ← generated previews (optional)
```

**No blob duplication at Tier 1/2.** The NAS path is stored in PostgreSQL and served directly. At Tier 3, MinIO mirrors hot files for HA.

### 5.2 PostgreSQL Schema

```sql
-- Core file registry
CREATE TABLE files (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    sha256          CHAR(64) NOT NULL,
    size            BIGINT NOT NULL,
    mime            TEXT NOT NULL,
    encoding        TEXT,                          -- UTF-8, ASCII, etc.
    created_at      TIMESTAMPTZ DEFAULT now(),
    modified_at     TIMESTAMPTZ,                   -- NAS file mtime
    indexed_at      TIMESTAMPTZ,
    reindexed_at    TIMESTAMPTZ,
    status          TEXT DEFAULT 'pending'
                    CHECK (status IN ('pending','indexing','indexed','failed','deleted')),
    schema_id       UUID REFERENCES schemas(id),
    ai_summary      TEXT,                          -- LLM-generated summary
    ai_tags         TEXT[],                        -- LLM-classified tags
    user_tags       TEXT[],                        -- user-assigned tags
    metadata        JSONB DEFAULT '{}',            -- arbitrary key-value
    embedding       vector(768)                    -- pgvector for semantic search
);

-- Multiple paths can point to the same content (dedup)
CREATE TABLE file_paths (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id         UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    nas_path        TEXT NOT NULL UNIQUE,           -- /volume1/data/configs/foo.json
    discovered_at   TIMESTAMPTZ DEFAULT now(),
    last_seen_at    TIMESTAMPTZ DEFAULT now(),
    is_primary      BOOLEAN DEFAULT true
);

-- Inferred document schemas (deduplicated across similar files)
CREATE TABLE schemas (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name            TEXT,                           -- auto-generated or user-named
    mime            TEXT NOT NULL,
    schema_json     JSONB NOT NULL,                 -- JSON Schema representation
    schema_hash     CHAR(64) NOT NULL UNIQUE,       -- SHA-256 of canonical schema
    file_count      INTEGER DEFAULT 0,              -- how many files match this schema
    sample_file_id  UUID REFERENCES files(id),
    created_at      TIMESTAMPTZ DEFAULT now(),
    description     TEXT                            -- LLM-generated description
);

-- File version history (tracks changes over time)
CREATE TABLE file_versions (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id         UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    sha256          CHAR(64) NOT NULL,
    size            BIGINT NOT NULL,
    captured_at     TIMESTAMPTZ DEFAULT now(),
    diff_summary    TEXT                            -- LLM-generated change summary
);

-- Structured field index for JSONPath/XPath queries
CREATE TABLE extracted_fields (
    id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_id         UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
    field_path      TEXT NOT NULL,                  -- e.g. "$.database.host" or "//server/@name"
    field_type      TEXT NOT NULL,                  -- string, number, boolean, array, object
    value_text      TEXT,                           -- string representation
    value_numeric   DOUBLE PRECISION,               -- if numeric
    value_boolean   BOOLEAN                         -- if boolean
);

-- Indexes
CREATE INDEX idx_files_sha256 ON files(sha256);
CREATE INDEX idx_files_status ON files(status);
CREATE INDEX idx_files_mime ON files(mime);
CREATE INDEX idx_files_user_tags ON files USING GIN (user_tags);
CREATE INDEX idx_files_ai_tags ON files USING GIN (ai_tags);
CREATE INDEX idx_files_metadata ON files USING GIN (metadata);
CREATE INDEX idx_files_embedding ON files USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
CREATE INDEX idx_file_paths_nas ON file_paths(nas_path);
CREATE INDEX idx_file_paths_file ON file_paths(file_id);
CREATE INDEX idx_extracted_fields_path ON extracted_fields(field_path, file_id);
CREATE INDEX idx_extracted_fields_value ON extracted_fields(field_path, value_text);
CREATE INDEX idx_schemas_hash ON schemas(schema_hash);
```

### 5.3 Meilisearch Index Configuration (Tier 1/2)

```json
{
  "uid": "files",
  "primaryKey": "id",
  "searchableAttributes": [
    "content",
    "name",
    "ai_summary",
    "user_tags",
    "ai_tags",
    "metadata"
  ],
  "filterableAttributes": [
    "mime",
    "user_tags",
    "ai_tags",
    "schema_id",
    "size",
    "indexed_at",
    "status"
  ],
  "sortableAttributes": [
    "indexed_at",
    "size",
    "name"
  ],
  "faceting": {
    "maxValuesPerFacet": 100
  },
  "pagination": {
    "maxTotalHits": 10000
  },
  "typoTolerance": {
    "enabled": true,
    "minWordSizeForTypos": {
      "oneTypo": 4,
      "twoTypos": 8
    }
  }
}
```

### 5.4 OpenSearch Mapping (Tier 3)

Same structure as v1 but with the addition of:

```json
{
  "mappings": {
    "properties": {
      "ai_summary": { "type": "text", "analyzer": "standard" },
      "ai_tags": { "type": "keyword" },
      "user_tags": { "type": "keyword" },
      "extracted_fields": {
        "type": "nested",
        "properties": {
          "path": { "type": "keyword" },
          "value": { "type": "text" },
          "numeric_value": { "type": "double" }
        }
      },
      "embedding": {
        "type": "knn_vector",
        "dimension": 768,
        "method": {
          "name": "hnsw",
          "space_type": "cosinesimil",
          "engine": "nmslib"
        }
      }
    }
  }
}
```

---

## 6. Scaling Strategy

### 6.1 Tier Transition Triggers

| Metric | Tier 1 → 2 Trigger | Tier 2 → 3 Trigger |
|--------|--------------------|--------------------|
| **Indexed files** | > 500K | > 5M |
| **Total data size** | > 500 GB | > 5 TB |
| **Concurrent clients** | > 10 | > 50 |
| **Search QPS** | > 20 | > 200 |
| **Ingestion rate** | > 1 GB/hr | > 10 GB/hr |

### 6.2 Scaling Strategies by Tier

**Tier 1 (Single Node)**
- Vertical scaling only: add RAM for Meilisearch, tune PostgreSQL shared_buffers
- Meilisearch handles up to ~10M documents on a single node with 16 GB RAM
- PostgreSQL with pgvector handles ~1M embeddings with ivfflat index

**Tier 2 (K3s, 2-5 Nodes)**
- SISS API: multiple replicas behind K3s Service load balancer
- Ingestion workers: scale horizontally via NATS consumer groups
- Meilisearch: single instance (still sufficient), or federated multi-index
- PostgreSQL: primary + read replica for search queries
- Redis: single instance with persistence, or Valkey cluster mode
- NATS: 3-node cluster for HA

**Tier 3 (K8s, 5+ Nodes)**
- OpenSearch: 3+ data nodes, sharded indexes, replica factor 1-2
- MinIO: erasure-coded distributed mode (4+2 nodes minimum)
- PostgreSQL: Patroni HA with streaming replication + read replicas
- SISS API + MCP: HPA based on CPU and request latency
- Ingestors: HPA based on NATS queue depth

### 6.3 NAS Bandwidth Considerations

The NAS is the bottleneck at every tier. Plan accordingly:

| NAS Link | Max Throughput | Files/min (avg 50 KB) | Recommendation |
|----------|---------------|-----------------------|----------------|
| 1 GbE | ~110 MB/s | ~2,200 | Sufficient for Tier 1 |
| 2.5 GbE | ~280 MB/s | ~5,600 | Recommended for Tier 2 |
| 10 GbE | ~1.1 GB/s | ~22,000 | Required for Tier 3 |
| Link Aggregation (2x1G) | ~200 MB/s | ~4,000 | Budget Tier 2 option |

---

## 7. Security

### 7.1 Tier 1 (MVP — Minimal but Secure)

| Layer | Approach |
|-------|----------|
| **Transport** | Caddy auto-TLS (self-signed or Let's Encrypt for `.home.arpa`) |
| **Authentication** | API key in `Authorization: Bearer <key>` header. Keys stored bcrypt-hashed in PostgreSQL. |
| **Authorization** | Simple role-based: `admin` (full access), `editor` (read + upload), `reader` (read only) |
| **Rate Limiting** | Caddy rate limiting (per-IP, per-key) |
| **NAS Mount** | `nosuid,nodev,noexec` for read paths; separate mount with write for uploads |
| **Input Validation** | Rust type system + serde validation at deserialization boundary |

### 7.2 Tier 2/3 (Full Security Stack)

Inherits all v1 security measures (Keycloak, OPA, mTLS, encryption at rest, audit logging, SBOM/Trivy) plus:

| Addition | Details |
|----------|---------|
| **MCP Auth** | API keys or OAuth2 tokens passed in MCP connection handshake. Per-tool authorization via OPA. |
| **File-Level ACL** | PostgreSQL-backed ACLs: each file can have owner, group, and permission bits. OPA evaluates on every access. |
| **Content Scanning** | Optional ClamAV scan on upload before indexing (prevent malware storage). |
| **Signed Downloads** | Time-limited HMAC-signed URLs for file downloads (prevents URL sharing). |

---

## 8. Deployment

### 8.1 Tier 1: Docker Compose

```yaml
# docker-compose.yml
services:
  siss:
    image: ghcr.io/your-org/siss:latest
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"   # REST API
      - "3000:3000"   # MCP SSE
    environment:
      DATABASE_URL: postgres://siss:secret@postgres:5432/siss
      MEILI_URL: http://meilisearch:7700
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}
      NAS_ROOT: /mnt/nas/data
      PEGASUS_URL: http://pegasus.home.arpa:8000
      STELLA_URL: http://stella.home.arpa:8000
      LLM_ENABLED: "true"
    volumes:
      - nas-data:/mnt/nas/data:ro          # read-only for indexing
      - nas-uploads:/mnt/nas/uploads:rw    # read-write for uploads
    depends_on:
      - postgres
      - meilisearch

  postgres:
    image: pgvector/pgvector:pg17
    environment:
      POSTGRES_DB: siss
      POSTGRES_USER: siss
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  meilisearch:
    image: getmeili/meilisearch:v1.13
    environment:
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}
      MEILI_ENV: production
    volumes:
      - meili-data:/meili_data
    ports:
      - "7700:7700"

  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy-data:/data

volumes:
  pgdata:
  meili-data:
  caddy-data:
  nas-data:
    driver: local
    driver_opts:
      type: nfs4
      o: addr=flashstore.home.arpa,rw,hard,intr,noatime,nofail,rsize=1048576,wsize=1048576
      device: ":/volume1/data"
  nas-uploads:
    driver: local
    driver_opts:
      type: nfs4
      o: addr=flashstore.home.arpa,rw,hard,intr,noatime,nofail,rsize=1048576,wsize=1048576
      device: ":/volume1/uploads"
```

**Caddyfile:**

```
siss.home.arpa {
    reverse_proxy /api/* siss:8080
    reverse_proxy /mcp/* siss:3000

    rate_limit {
        zone api {
            key {remote_host}
            events 100
            window 1m
        }
    }

    log {
        output file /var/log/caddy/access.log
    }
}
```

### 8.2 Running on Synology DSM

The Docker Compose stack can run directly on the Synology NAS itself using Synology's Container Manager (Docker) package:

1. Install **Container Manager** from DSM Package Center
2. Clone the SISS repo to a shared folder
3. Run `docker compose up -d` via SSH or Container Manager UI
4. NAS volumes become local bind mounts (no NFS needed — direct `/volume1/data` access)

This eliminates the NFS bottleneck entirely for Tier 1.

### 8.3 Tier 2: K3s with Helm

```bash
# Bootstrap K3s cluster
curl -sfL https://get.k3s.io | sh -

# Add worker nodes
curl -sfL https://get.k3s.io | K3S_URL=https://master:6443 K3S_TOKEN=<token> sh -

# Deploy SISS
helm install siss ./charts/siss \
  --set replicas.api=2 \
  --set replicas.ingestor=2 \
  --set nats.enabled=true \
  --set redis.enabled=true \
  --set nas.server=flashstore.home.arpa \
  --set nas.path=/volume1/data
```

---

## 9. Performance Targets

| Metric | Tier 1 | Tier 2 | Tier 3 |
|--------|--------|--------|--------|
| **Indexed files** | 500K | 5M | 50M+ |
| **Search latency (p95)** | < 50 ms | < 100 ms | < 200 ms |
| **Ingestion rate** | 500 files/min | 5K files/min | 50K files/min |
| **Concurrent clients** | 10 | 50 | 500+ |
| **Search QPS** | 50 | 200 | 2,000+ |
| **Upload throughput** | 100 MB/min | 1 GB/min | 5 GB/min |
| **Time to first search result** | < 30 ms | < 50 ms | < 100 ms |
| **Embedding generation** | ~100 files/min (Stella) | ~100 files/min | ~500 files/min (batched) |
| **Memory (total stack)** | < 2 GB | < 8 GB | < 64 GB |

---

## 10. Implementation Roadmap

### Phase 0: Foundation (1 week)

- Git repo setup with CI (GitHub Actions)
- Rust project scaffolding (Cargo workspace: `siss-core`, `siss-api`, `siss-mcp`, `siss-ingest`)
- Docker Compose with PostgreSQL + Meilisearch
- Basic health check endpoints

### Phase 1: Core Ingestion + Search (3 weeks)

- NFS mount watcher (inotify + periodic scan)
- SHA-256 dedup pipeline
- ASCII/JSON/XML parsers (serde_json, quick-xml)
- Schema inference and registry
- Meilisearch indexing pipeline
- Basic `GET /search?q=` and `GET /files/{id}` endpoints
- **Milestone: search your NAS files from a browser**

### Phase 2: Full REST API + Structured Queries (2 weeks)

- Complete CRUD endpoints (upload, delete, update metadata)
- JSONPath query engine (`jsonpath-rust` crate)
- XPath query engine (`sxd-xpath` crate)
- Faceted search and filtering
- Pagination, sorting
- OpenAPI spec generation (utoipa crate)

### Phase 3: MCP Server (2 weeks)

- MCP server implementation (stdio + SSE transport)
- All 11 tools from Section 4.2
- Resources and prompts
- Integration testing with Claude Desktop / Claude Code
- **Milestone: ask Claude to search your NAS files**

### Phase 4: LLM Integration (2 weeks)

- Embedding generation via Stella API (batch pipeline)
- pgvector storage and semantic search endpoint
- Summary generation via Pegasus API (async queue)
- Hybrid search with RRF re-ranking
- **Milestone: semantic search across your data corpus**

### Phase 5: Security + Polish (2 weeks)

- API key authentication
- Role-based access control
- Caddy TLS + rate limiting
- WebSocket/SSE for real-time index update notifications
- CLI tool (`siss-cli`) for admin operations
- Documentation (API docs, deployment guide)
- **Milestone: v1.0.0 release**

### Phase 6: Scale (4 weeks, as needed)

- NATS event bus integration
- Redis caching layer
- K3s Helm charts
- Horizontal scaling for API + ingestors
- Load testing with k6

**Total to MVP: 6-8 weeks. Total to production: 16-20 weeks.**

---

## 11. Rust Crate Dependencies (Key Selections)

| Crate | Purpose |
|-------|---------|
| `axum` | HTTP framework (REST API) |
| `rmcp` | MCP server implementation (JSON-RPC 2.0) |
| `serde` + `serde_json` | JSON serialization/deserialization |
| `quick-xml` | Fast XML parsing with serde integration |
| `sqlx` | Async PostgreSQL driver with compile-time query checking |
| `pgvector` | pgvector support for sqlx |
| `meilisearch-sdk` | Meilisearch client |
| `jsonpath-rust` | JSONPath query evaluation |
| `sxd-xpath` + `sxd-document` | XPath query evaluation |
| `notify` | Cross-platform filesystem watcher (inotify on Linux) |
| `sha2` | SHA-256 hashing |
| `tokio` | Async runtime |
| `tower` | Middleware (rate limiting, auth, logging) |
| `utoipa` | OpenAPI spec generation from code |
| `tracing` | Structured logging and diagnostics |
| `reqwest` | HTTP client (for LLM API calls) |

---

## 12. Summary

### What changed from v1

The v2 design makes three fundamental shifts:

1. **Start simple, scale when needed.** The Docker Compose MVP runs on the Synology NAS itself or any single Linux host. No Kubernetes, no Istio, no Kafka on day one. Clear trigger metrics tell you when to promote tiers.

2. **NAS as source of truth.** Files stay on the NAS. No wasteful duplication into MinIO until you need cross-node HA at Tier 3. The index *points to* files rather than *copying* them.

3. **Correct MCP implementation.** The MCP server follows Anthropic's actual Model Context Protocol — JSON-RPC 2.0 over stdio/SSE, with proper tool definitions, resources, and prompts. AI agents can natively search, retrieve, and upload files.

### What's new in v2

- **Local LLM pipeline**: Leverages your existing Pegasus (120B, summaries/classification) and Stella (8B, embeddings) for semantic understanding
- **Content-addressable dedup**: SHA-256 based, saves re-processing identical files
- **XPath + JSONPath native queries**: First-class structured query support, not just full-text
- **Hybrid search with RRF**: Combines keyword, structured, and semantic results
- **File versioning**: Tracks changes to files over time
- **Real-time notifications**: WebSocket/SSE for live index updates
- **Rust single-binary**: ~10 MB memory footprint, no JVM, no GC pauses, one artifact to deploy
- **6-8 week MVP**: Usable search in 4 weeks, full API in 6, MCP in 8
