# Local Agentic Software Development Platform 

This repository contains documentation and configuration for deploying a **production dual-GPU AI infrastructure** with self-hosted Open Source LLMs on NVIDIA Blackwell hardware (GB10/Grace Blackwell).

All AI inference runs locally on your own GPU hardware, giving you full control, privacy, and no API costs.

## ğŸ¯ Current Deployment

**Two production systems serving complementary roles:**

- **Pegasus** - [GPT-OSS-120B](systems/pegasus/) (117B params) - Architect & Analyst
- **Stella** - [Qwen3-8B](systems/stella/) (8.2B dense) - General-Purpose

ğŸ“Š **[View Current Status â†’](STATUS.md)** | ğŸ“– **[View Changelog â†’](CHANGELOG.md)**

## ğŸš€ Production Systems

### Pegasus - GPT-OSS-120B
**Status**: âœ… Operational | **API**: http://pegasus.home.arpa:8000

- **Model**: OpenAI GPT-OSS-120B (117B params, MXFP4 quantized)
- **Performance**: 34 tokens/sec sustained
- **Context**: 131,072 tokens
- **Role**: Long-context analysis, architecture design, code review
- **Features**: OpenAI-compatible tool calling

ğŸ“– **[Documentation â†’](systems/pegasus/)** | ğŸš€ **[Quick Start â†’](systems/pegasus/QUICKSTART.md)**

### Stella - Qwen3-8B
**Status**: âœ… Operational | **API**: http://stella.home.arpa:8000

- **Model**: Qwen3-8B (8.2B dense, Q8_0 GGUF)
- **Performance**: 27.8 tok/s generation, 2,236 tok/s prompt
- **Context**: 32,768 tokens
- **Role**: General-purpose inference, fast responses
- **Features**: OpenAI-compatible API, thinking mode

ğŸ“– **[Documentation â†’](systems/stella/)** | ğŸš€ **[Quick Start â†’](systems/stella/QUICKSTART.md)**

## ğŸ“Š System Comparison

| Feature | Pegasus | Stella |
|---------|---------|--------|
| **Hardware** | ASUS Ascent GX10 | Lenovo ThinkStation PGX |
| **GPU Memory** | 128 GB | 128 GB (unified ARM) |
| **API Endpoint** | http://pegasus.home.arpa:8000 | http://stella.home.arpa:8000 |
| **Model** | OpenAI GPT-OSS-120B | Qwen3-8B |
| **Parameters** | 117B MoE | 8.2B dense |
| **Model Size** | 59 GiB (MXFP4 GGUF) | 8.1 GiB (Q8_0 GGUF) |
| **Model Storage** | NFS (flashstore) | NFS (flashstore) |
| **Quantization** | MXFP4 | Q8_0 |
| **Max Context** | 131,072 tokens | 32,768 tokens |
| **Speed** | 58.8 tok/s | 27.8 tok/s |
| **Engine** | llama.cpp (systemd) | llama.cpp (systemd) |
| **Reasoning Traces** | Yes (`reasoning_content`) | Thinking mode (Qwen3) |
| **OpenAI SDK Compatible** | Full | Full |
| **Specialization** | Architecture, analysis, code review | General-purpose, fast inference |

## ğŸ”§ Tool Calling

Both systems support tool/function calling with different formats:

### Pegasus (OpenAI Format)
Returns structured JSON in the `tool_calls` array - fully compatible with OpenAI SDKs:
```json
{
  "tool_calls": [{
    "function": {
      "name": "get_weather",
      "arguments": "{\"location\": \"Tokyo\"}"
    }
  }],
  "reasoning_content": "User asks about weather, using get_weather tool."
}
```

### Stella (Hermes Format)
Returns XML-formatted tool calls in the `content` field - requires custom parsing:
```xml
<tool_call>
<function=get_weather>
<parameter=location>Tokyo</parameter>
</function>
</tool_call>
```

### Hermes Format Documentation
For implementing a Hermes parser, see the official NousResearch documentation:
- [Hermes Function Calling (GitHub)](https://github.com/NousResearch/Hermes-Function-Calling) - Reference implementation
- [Hermes Dataset & Format Spec](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1) - Format specification
- [Hermes 3 Technical Report](https://arxiv.org/pdf/2408.11857) - Detailed documentation
- [Parser Implementation (functioncall.py)](https://github.com/NousResearch/Hermes-Function-Calling/blob/main/functioncall.py) - Python parser example

## ğŸ“¦ Projects

### [vLLM for NVIDIA GB10](vllm-gb10/)

Self-contained vLLM project with native GB10/Blackwell support. This is the foundation for both Pegasus and Stella deployments.

**Features:**
- Native GB10/Blackwell support (SM 12.1)
- Multi-architecture CUDA kernels
- OpenAI-compatible API
- Docker-based deployment
- Comprehensive documentation

ğŸ“– **[vLLM GB10 Project â†’](vllm-gb10/)**

## ğŸ—ï¸ Architecture

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Frontend / Clients        â”‚
                 â”‚   - OpenCode / Cursor       â”‚
                 â”‚   - API clients             â”‚
                 â”‚   - SSH access              â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       OpenAI API & SSH        â”‚
                â”‚                               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     Pegasus      â”‚           â”‚      Stella      â”‚
       â”‚  ASUS Ascent     â”‚           â”‚  Lenovo PGX      â”‚
       â”‚  GX10 (128GB)    â”‚           â”‚  GB10 (128GB)    â”‚
       â”‚                  â”‚           â”‚  Grace Blackwell â”‚
       â”‚  GPT-OSS-120B    â”‚           â”‚  Qwen3-8B        â”‚
       â”‚  117B params     â”‚           â”‚  8.2B dense      â”‚
       â”‚  58.8 tok/s      â”‚           â”‚  27.8 tok/s      â”‚
       â”‚  :8000           â”‚           â”‚  :8000           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                               â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   NFS Model Storage  â”‚
                    â”‚  flashstore.arpa     â”‚
                    â”‚  9.1TB RAID5         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Documentation Structure

```
â”œâ”€â”€ STATUS.md                    # Current deployment status
â”œâ”€â”€ CHANGELOG.md                 # Project timeline and changes
â”œâ”€â”€ systems/                     # Per-system documentation
â”‚   â”œâ”€â”€ pegasus/                # GPT-OSS-120B deployment
â”‚   â””â”€â”€ stella/                 # Qwen3-8B deployment
â”œâ”€â”€ docs/                       # Supporting documentation
â”‚   â”œâ”€â”€ deployment/             # Deployment guides
â”‚   â”œâ”€â”€ network/                # Network configuration
â”‚   â”œâ”€â”€ archive/                # Historical documents
â”‚   â””â”€â”€ research/               # Future projects
â”œâ”€â”€ vllm-gb10/                  # vLLM GB10 project (self-contained)
â””â”€â”€ scripts/                    # Utility scripts
```

### ğŸ“– Key Documents

**System Documentation**:
- [Pegasus (GPT-OSS-120B)](systems/pegasus/) - Production Architect & Analyst system
- [Stella (Qwen3-8B)](systems/stella/) - General-purpose fast inference

**Status & History**:
- [STATUS.md](STATUS.md) - Current deployment status
- [CHANGELOG.md](CHANGELOG.md) - Project timeline

**Deployment**:
- [vLLM GB10 Setup Guide](vllm-gb10/SETUP.md) - Complete GB10 build guide

## ğŸš€ Quick Start

### Check System Status
```bash
# View current deployment status
cat STATUS.md

# Check Pegasus
curl http://pegasus.home.arpa:8000/health

# Check Stella (when deployed)
curl http://stella.home.arpa:8000/health
```

### Using the APIs

See individual system documentation:
- **Pegasus**: [systems/pegasus/QUICKSTART.md](systems/pegasus/QUICKSTART.md)
- **Stella**: [systems/stella/QUICKSTART.md](systems/stella/QUICKSTART.md)

## Contributing

Contributions welcome! See [vllm-gb10/CONTRIBUTING.md](vllm-gb10/CONTRIBUTING.md).

## License

MIT License - see [LICENSE](LICENSE) for details.

Individual components may have their own licenses:
- vLLM: Apache 2.0
- Docker images: Based on official vLLM images
