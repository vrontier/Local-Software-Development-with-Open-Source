# Storage-Index-Search System (SISS) Design Specification

> Generated by GPT-OSS-120B (MXFP4) on Pegasus — 2026-02-12

## Performance Metrics

| Metric | Value |
|--------|-------|
| **Model** | GPT-OSS-120B (MXFP4, 117B MoE) |
| **Prompt tokens** | 480 |
| **Completion tokens** | 6,618 |
| **Total tokens** | 7,098 |
| **Prompt processing** | 411 ms (1,167 tok/s) |
| **Generation speed** | 45.3 tok/s |
| **Time per token** | 22.1 ms |
| **Total generation time** | 146.0 s |
| **Wall clock** | 146.8 s |

---

## 1. High-Level Architecture (textual diagram)

```
+---------------------------------------------------------------+
|                         CLIENT LAYER                         |
|  ─────────────────────────────────────────────────────────── |
|  Web UI / CLI          REST JSON API          MCP (gRPC)   |
|  Human users           (HTTPS)               (gRPC/TLS)   |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                     API GATEWAY / EDGE                        |
|  - Nginx (reverse-proxy, TLS termination)                     |
|  - Rate-limiter & auth (OAuth2 / OIDC)                        |
|  - OpenAPI validation, request routing                        |
+---------------------------+-----------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                       SERVICE MESH (Istio)                   |
|  - Mutual TLS, observability, traffic-shaping                 |
+---------------------------+-----------------------------------+
                            |
      +---------------------+---------------------+---------------------+
      |                     |                     |                     |
      v                     v                     v                     v
+-----------+        +------------+        +------------+        +------------+
| Ingestor  |        | Parser &   |        | Indexer    |        | Search API |
| Service   |        | Schema     |        | Service    |        | Service    |
| (Go)      |        | Extractor  |        | (Java)     |        | (Go)       |
+-----------+        +------------+        +------------+        +------------+
      |                     |                     |                     |
      v                     v                     v                     v
+---------------------------------------------------------------+
|                     Distributed Data Plane                     |
|  ┌─────────────────────┐   ┌─────────────────────┐   ┌───────┐ |
|  │  Object Store (S3)  │   │  Search Engine      │   │  DB   │ |
|  │  MinIO (cluster)    │   │  OpenSearch (ES-7)  │   │  PostgreSQL │ |
|  └─────────────────────┘   └─────────────────────┘   └───────┘ |
+---------------------------------------------------------------+
                            |
                            v
+---------------------------------------------------------------+
|                     BACKING FILE STORE                         |
|  Synology NAS – NFS/SMB share (read-only for ingest,          |
|  read/write for upload/download)                               |
+---------------------------------------------------------------+
```

*All services are containerised and deployed on a Kubernetes (K8s) cluster (or K3s for small sites).*

---

## 2. Component Breakdown & Technology Choices

| Layer / Component | Technology (open-source) | Rationale |
|-------------------|--------------------------|-----------|
| **Ingress / API Gateway** | **NGINX Ingress Controller** + **Kong** (optional) | Mature TLS termination, request validation, rate-limiting, OAuth2/OIDC plugins. |
| **Service Mesh** | **Istio** (or **Linkerd**) | Zero-trust mTLS, observability (Jaeger, Prometheus), traffic shaping for bulk operations. |
| **Ingestion Service** | **Go** (goroutine-friendly) | Low-latency, easy concurrency, native NFS/SMB client via `cifs` mount. |
| **Parser & Schema Extractor** | **Apache Tika** (Java) + **Jackson** (JSON) + **XOM** (XML) | Proven content detection, robust MIME handling, schema inference (Tika-XML, Tika-JSON). |
| **Indexing Engine** | **OpenSearch** (fork of Elasticsearch) | Distributed inverted index, per-field indexing, vector search (optional), open-source licence. |
| **Object Store for Blobs** | **MinIO** (S3-compatible) | Handles large binary files, supports erasure coding, works on-prem. |
| **Metadata DB** | **PostgreSQL** (with **PG-Vector** extension optional) | Relational metadata, ACID guarantees, easy joins for complex filters. |
| **Search API Service** | **Go (Gin / Echo)** | Fast HTTP handling, easy OpenAPI generation. |
| **MCP Server** | **gRPC (Go)** + **Protocol Buffers** | Binary-efficient, streaming support for bulk upload/download, native for LLM toolkits. |
| **Authentication / Authorization** | **Keycloak** (OIDC) + **OPA** (Open Policy Agent) | Centralised identity, fine-grained ABAC/ RBAC policies. |
| **Observability** | **Prometheus** + **Grafana** + **Jaeger** | Metrics, dashboards, distributed tracing. |
| **Orchestration** | **Kubernetes** (or **K3s** for edge) + **Helm** charts | Declarative deployment, auto-scaling, rolling upgrades. |
| **CI/CD** | **GitHub Actions** + **Argo CD** | Automated builds, Git-Ops rollout. |

All components are licensed under permissive OSS licences (Apache-2.0, MIT, BSD) and can be self-hosted on commodity hardware.

---

## 3. Data Flow

### 3.1 Ingestion & Indexing

1. **File Discovery**
   - A scheduled **Ingestor** watches the NAS mount (NFS/SMB).
   - New/changed files are detected via `inotify` (Linux) or periodic directory scans.

2. **Metadata Capture**
   - For each file: compute SHA-256 hash, size, timestamps, owner, path.
   - Insert a row into `files` table (PostgreSQL) → `file_id`.

3. **Blob Storage**
   - The raw file is streamed into MinIO (`PUT /bucket/<file_id>`).
   - The NAS copy can be optionally archived or removed after successful upload.

4. **Content Type Detection**
   - Ingestor calls **Apache Tika** (via REST or embedded) to detect MIME type.

5. **Parsing**
   - **Text** → tokenisation, language detection (fastText).
   - **XML** → DOM parsing → extract element hierarchy, attributes, text nodes.
   - **JSON** → recursive walk → infer field types, detect nested objects/arrays.

6. **Schema Extraction**
   - Generate a **canonical schema** (JSON-Schema like) for each document.
   - Store schema metadata in `schemas` table; link via `file_id`.

7. **Indexing**
   - Build **field-level** inverted indexes in OpenSearch:
     - `content` (full-text) – analyzed with standard analyzer + n-gram for autocomplete.
     - `json.*` / `xml.*` – keyword / numeric / date sub-fields using dynamic mapping.
   - Store **vector embeddings** (optional) using `sentence-transformers` (BERT) for semantic similarity.
   - Index document ID = `file_id`; store pointers to MinIO object and schema version.

8. **Completion**
   - Update `files` status = `indexed`.
   - Emit an event to Kafka (optional) for downstream notifications.

### 3.2 Search & Retrieval

1. **Client Request** → **API Gateway** → **Search API Service** (REST) or **MCP Server** (gRPC).
2. **Authorization** → OIDC token → OPA policy evaluation.
3. **Query Translation**
   - Full-text query → OpenSearch `match`/`multi_match`.
   - Structured query → OpenSearch `nested`/`term` filters on `json.*` / `xml.*` fields.
   - Facets → OpenSearch aggregations.
   - Vector similarity → `knn` query (if enabled).
4. **Result Set** → IDs + highlights + metadata.
5. **Pagination** → `search_after` (deep pagination) or `from/size` for shallow pages.
6. **Download**
   - For a single file: presigned MinIO URL (`GET /download/{file_id}`) → streamed to client.
   - For bulk: MCP server streams a tar/zip archive, or REST returns a signed URL to a pre-generated archive in MinIO.
7. **Upload**
   - POST `/files` (multipart) or gRPC `UploadChunk` streaming.
   - Files are written to NAS (via CIFS mount) **and** uploaded to MinIO, then indexed as above.

---

## 4. API Specification

### 4.1 REST JSON API (OpenAPI 3.1)

| Method | Path | Description | Request Body | Response |
|--------|------|-------------|--------------|----------|
| **GET** | `/api/v1/files` | List files / metadata (filter, pagination, faceting) | Query params: `q`, `filter`, `sort`, `page`, `size`, `facets` | `{items:[...], total, facets:{...}}` |
| **GET** | `/api/v1/files/{id}` | Get metadata for a single file | – | `{id, name, size, mime, schemaId, indexedAt, tags, ...}` |
| **GET** | `/api/v1/files/{id}/download` | Redirect to presigned URL (or stream) | – | `302 Location: https://minio/...` |
| **POST** | `/api/v1/files` | Upload one or many files (multipart) | `multipart/form-data` (files, optional `metadata` JSON) | `{ids:[...]}` |
| **POST** | `/api/v1/files/batch` | Bulk upload via JSON array of base64 or URLs | `{files:[{name, contentBase64, metadata}]}` | `{ids:[...]}` |
| **GET** | `/api/v1/search` | Full-text / structured search | Query params: `q`, `json.field=value`, `xml.path=value`, `vector=...`, `sort`, `page`, `size`, `facets` | Same shape as list endpoint, plus `highlights` |
| **POST** | `/api/v1/search` | Complex query DSL (JSON) | `{query:{bool:{must:[...] , filter:[...] }}, aggs:{...}}` | Same as GET |
| **DELETE** | `/api/v1/files/{id}` | Delete file (remove from MinIO, DB, index) | – | `204 No Content` |
| **PATCH** | `/api/v1/files/{id}` | Update mutable metadata (tags, description) | `{tags:["a","b"], description:"..."}` | Updated object |
| **GET** | `/api/v1/schemas/{id}` | Retrieve inferred schema (JSON-Schema) | – | JSON schema object |

**Common Query Parameters**

- `q` – free-text query (default field `content`).
- `filter` – URL-encoded JSON filter for structured fields.
- `sort` – comma-separated list `field:asc|desc`.
- `facets` – comma-separated list of fields to aggregate.
- `page` / `size` – pagination (default size 20).

**Error Model**

```json
{
  "error": {
    "code": "ERR_VALIDATION",
    "message": "Invalid query syntax",
    "details": [...]
  }
}
```

### 4.2 MCP (Model Context Protocol)

A lightweight gRPC service designed for LLM agents that need streaming and batch operations.

```proto
syntax = "proto3";

package siss.mcp;

service Store {
  // Search – returns a stream of Result messages
  rpc Search (SearchRequest) returns (stream SearchResult);

  // Retrieve a file (streamed bytes)
  rpc Download (DownloadRequest) returns (stream Chunk);

  // Upload – client streams chunks, server returns FileId
  rpc Upload (stream UploadChunk) returns (UploadResponse);

  // BulkUpload – client streams multiple files with metadata
  rpc BulkUpload (stream BulkUploadChunk) returns (BulkUploadResponse);

  // GetMetadata – single request/response
  rpc GetMetadata (MetadataRequest) returns (MetadataResponse);

  // Delete a file
  rpc Delete (DeleteRequest) returns (DeleteResponse);
}

/* ---- Messages ---- */

message SearchRequest {
  string query = 1;                // full-text
  map<string, string> filters = 2; // structured field filters
  repeated string facets = 3;
  int32 page = 4;
  int32 size = 5;
  string sort = 6;                 // e.g. "created_at:desc"
  // optional vector embedding for semantic search
  repeated float vector = 7;
}

message SearchResult {
  string file_id = 1;
  string title = 2;
  string snippet = 3;
  map<string, string> highlights = 4;
  map<string, string> metadata = 5;
}

message DownloadRequest { string file_id = 1; }

message Chunk { bytes data = 1; }

message UploadChunk {
  oneof payload {
    FileInfo info = 1;   // name, mime, optional metadata
    bytes data = 2;      // raw bytes
  }
}
message FileInfo {
  string name = 1;
  string mime = 2;
  map<string, string> metadata = 3;
}
message UploadResponse { string file_id = 1; }

message BulkUploadChunk {
  oneof payload {
    FileInfo info = 1;
    bytes data = 2;
    EndOfFile eof = 3;
  }
}
message EndOfFile {}

message BulkUploadResponse {
  repeated string file_ids = 1;
}

message MetadataRequest { string file_id = 1; }
message MetadataResponse {
  string file_id = 1;
  string name = 2;
  string mime = 3;
  int64 size = 4;
  string created_at = 5;
  string indexed_at = 6;
  map<string, string> tags = 7;
  string schema_id = 8;
}

message DeleteRequest { string file_id = 1; }
message DeleteResponse { bool ok = 1; }
```

**Transport** – gRPC over TLS (mutual auth optional). Streaming enables LLMs to request partial results or upload large files without loading whole payload into memory.

---

## 5. Scaling Strategy

| Dimension | Technique | Details |
|-----------|-----------|---------|
| **Stateless Services** (Ingestor, Parser, Search API, MCP) | **Horizontal Pod Autoscaler (HPA)** based on CPU/Latency metrics. | Each replica is identical; Kubernetes Service load-balances. |
| **OpenSearch Cluster** | **Sharding** (default 5 shards per index) + **Replica** factor 1-2. | Add data nodes to increase shard count; re-balance using OpenSearch APIs. |
| **MinIO** | **Erasure-coded Distributed Mode** (4+2). | Scales linearly; each node adds capacity & throughput. |
| **PostgreSQL** | **Patroni** + **Streaming Replication** + **Read-replicas**. | Write traffic to primary; read-only metadata queries can be served by replicas. |
| **Kafka (optional)** | Event bus for ingestion notifications, back-pressure handling. | Partitioned topics; consumer groups for downstream processing. |
| **Cache Layer** | **Redis** (cluster mode) for query result caching, token bucket rate-limit counters. | TTL based on query hash. |
| **Bulk Operations** | **Back-pressure queues** (RabbitMQ or NATS) for large uploads before indexing. | Prevents spikes from overwhelming OpenSearch. |
| **Network** | Deploy services in the same VPC/subnet, use **CNI** with high-throughput (Calico/OVN). | Ensure NFS/SMB mount is on a high-speed LAN (10 GbE). |
| **Observability-Driven Autoscaling** | Combine Prometheus alerts with custom scaling rules (e.g., queue length > 10k). | Prevents SLA breaches. |

**Failure Domains** – each tier runs across at least three Kubernetes nodes (or three physical machines) to survive node loss. Data replication ensures no single-point-of-failure for indexes, blobs, and metadata.

---

## 6. Storage Layout & Index Schema Design

### 6.1 Object Store (MinIO)

```
bucket/
 └─ files/
     ├─ <file_id>                (raw binary)
     ├─ <file_id>.json           (optional extracted JSON view)
     └─ <file_id>.xml            (optional pretty-printed XML)
```

*Presigned URLs expire after configurable TTL (default 15 min).*

### 6.2 PostgreSQL Schema (simplified)

```sql
CREATE TABLE files (
    id            UUID PRIMARY KEY,
    name          TEXT NOT NULL,
    mime          TEXT NOT NULL,
    size          BIGINT,
    sha256        CHAR(64),
    created_at    TIMESTAMPTZ DEFAULT now(),
    indexed_at    TIMESTAMPTZ,
    schema_id     UUID REFERENCES schemas(id),
    tags          JSONB,
    metadata      JSONB
);

CREATE TABLE schemas (
    id            UUID PRIMARY KEY,
    source_mime   TEXT,
    schema_json   JSONB,               -- JSON-Schema representation
    generated_at  TIMESTAMPTZ DEFAULT now()
);

CREATE INDEX idx_files_mime ON files(mime);
CREATE INDEX idx_files_tags ON files USING GIN (tags);
```

### 6.3 OpenSearch Mapping (dynamic)

```json
{
  "mappings": {
    "properties": {
      "file_id": { "type": "keyword" },
      "content": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "keyword": { "type": "keyword" },
          "ngrams": {
            "type": "text",
            "analyzer": "edge_ngram_analyzer"
          }
        }
      },
      "json": {
        "type": "object",
        "dynamic": true,
        "properties": {
          "price": { "type": "double" },
          "date":  { "type": "date" },
          "status":{ "type": "keyword" }
        }
      },
      "xml": {
        "type": "object",
        "dynamic": true
      },
      "vector": {
        "type": "dense_vector",
        "dims": 768,
        "index": true,
        "similarity": "cosine"
      },
      "created_at": { "type": "date" },
      "indexed_at": { "type": "date" }
    }
  },
  "settings": {
    "analysis": {
      "analyzer": {
        "edge_ngram_analyzer": {
          "tokenizer": "edge_ngram_tokenizer",
          "filter": ["lowercase"]
        }
      },
      "tokenizer": {
        "edge_ngram_tokenizer": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": ["letter", "digit"]
        }
      }
    }
  }
}
```

*Dynamic mapping allows any new JSON/XML field to be indexed automatically. For fields that need keyword vs numeric semantics, the parser can pre-declare a mapping hint.*

---

## 7. Security Considerations

| Concern | Mitigation |
|---------|------------|
| **Authentication** | OIDC flow via Keycloak. All API calls require a Bearer JWT. |
| **Authorization** | OPA policies evaluate `subject`, `action`, `resource` (file-level RBAC). |
| **Transport Security** | TLS termination at NGINX; intra-cluster mTLS via Istio. |
| **Data-at-Rest Encryption** | MinIO server-side encryption (SSE-S3) + PostgreSQL Transparent Data Encryption (TDE). |
| **Network Segmentation** | Separate namespaces for public API, internal services, and storage. Use NetworkPolicies to restrict traffic. |
| **Rate Limiting & Quotas** | Kong/Nginx rate-limit plugin per API key; Redis token-bucket for per-user quotas (e.g., 10 GB upload/day). |
| **Input Validation** | OpenAPI schema validation; gRPC protobuf enforces field types. |
| **Audit Logging** | All requests logged to Loki/Elastic (centralised). Sensitive fields masked. |
| **Backup & Disaster Recovery** | MinIO erasure-coded bucket snapshots; PostgreSQL logical backups (pg_dump) + WAL archiving; OpenSearch snapshot repository to MinIO. |
| **Secure Mounts** | NAS share mounted with `nosuid, nodev, ro` for ingestion; write-only mount for uploads, enforced by POSIX ACLs. |
| **Dependency Management** | Use SBOM (Software Bill of Materials) generated by Syft; regularly scan images with Trivy. |

---

## 8. Deployment Architecture

### 8.1 Kubernetes Cluster (production)

```
Namespace: siss-system
 ├─ Deployment: ingress-nginx
 ├─ Deployment: keycloak
 ├─ Deployment: opa
 ├─ StatefulSet: opensearch (master + data nodes)
 ├─ StatefulSet: minio (distributed mode)
 ├─ Deployment: postgres (Patroni HA)
 ├─ Deployment: redis
 ├─ Deployment: ingest-service
 ├─ Deployment: parser-service
 ├─ Deployment: indexer-service
 ├─ Deployment: search-api
 ├─ Deployment: mcp-grpc
 ├─ Deployment: prometheus, grafana, jaeger
 └─ CronJob: re-indexer (for schema evolution)
```

All pods are **container-native** (Docker/OCI). Helm charts are provided for each component; values.yaml allows toggling between **single-node dev** and **multi-node prod**.

### 8.2 Edge / Small-Site Variant (K3s)

- Run a single-node K3s cluster on a VM.
- MinIO in **standalone** mode, OpenSearch in **single-node** mode.
- Use **Helm** with `--set replicaCount=1` for all services.
- Still mount the NAS via NFS/SMB from the host into the pod (`hostPath` or `nfs` volume).

### 8.3 CI/CD Pipeline

1. **GitHub Actions** builds Docker images, runs unit/integration tests.
2. **Trivy** scans images; fails on CVEs > 7 days.
3. **Argo CD** watches Helm chart repo → auto-sync to cluster.
4. **Canary Deployments** for API services (Istio traffic split 5/95).

---

## 9. Performance Targets & Capacity Planning

| Metric | Target (baseline) | Scaling Guidance |
|--------|-------------------|------------------|
| **Ingestion throughput** | 200 MB/min per ingestor pod (~3 GB/hr) | Add more ingest pods; ensure NFS bandwidth > 10 Gbps. |
| **Indexing latency** | ≤ 5 s from file arrival to searchable document (text); ≤ 10 s for large XML/JSON (>5 MB) | Increase parser replicas; allocate more heap to OpenSearch. |
| **Search QPS** | 100 req/s for simple full-text; 30 req/s for complex structured queries (≤ 200 ms latency) | Horizontal pod autoscale; add OpenSearch data nodes. |
| **Bulk download** | 5 GB/min (aggregate) using parallel MinIO presigned URLs | Use multi-part downloads; tune MinIO `concurrency` config. |
| **Upload** | 2 GB/min (aggregate) with streaming gRPC | Enable `maxRequestBodySize` in NGINX; increase `maxMessageSize` in gRPC. |
| **Storage** | 1 TB raw files → 1.2 TB MinIO (10% overhead for versioning) | Plan 2x raw size for replication + erasure coding overhead. |
| **Index size** | ~30% of raw text size (depends on field count) | Monitor OpenSearch disk usage; add shards when > 70% full. |
| **Latency SLA** | 95th-percentile ≤ 300 ms for search, ≤ 1 s for metadata fetch | Use Redis cache for hot queries; keep index warm. |

**Capacity Planning Steps**

1. **Estimate daily ingest volume** (GB/day).
2. **Compute required MinIO nodes**: `(total GB * replication factor) / node_capacity`.
3. **OpenSearch**: `shard_size ≈ 30 GB`. Number of primary shards = `total_index_size / shard_size`. Add 1 replica → double disk.
4. **Network**: Ensure NICs ≥ 10 GbE on NAS and cluster nodes; allocate 1 GbE for client-facing traffic.
5. **CPU / Memory**:
   - Ingestor/Parser: 1 vCPU per 100 MB/s processing.
   - OpenSearch data node: 8 vCPU, 32 GiB RAM (heap ≤ 16 GiB).
   - MinIO: 2 vCPU, 8 GiB per node.

---

## 10. Implementation Roadmap

| Phase | Duration | Milestones | Deliverables |
|-------|----------|------------|--------------|
| **0 – Foundations** | 2 weeks | Set up Git repo, CI pipeline. Deploy single-node K3s dev cluster. Pull in base Helm charts. | Repo, CI, dev environment. |
| **1 – Core Ingestion** | 4 weeks | Implement NFS/SMB mount handling. Build Ingestor service (Go). Store raw blobs in MinIO. Basic metadata DB schema. | Working ingest pipeline (file → MinIO + PostgreSQL). |
| **2 – Parsing & Schema Extraction** | 5 weeks | Integrate Apache Tika (REST) for MIME detection. Implement JSON/XML recursive parser. Auto-generate JSON-Schema objects. Store schemas in DB. | Parsed documents with schema metadata. |
| **3 – Indexing Engine** | 4 weeks | Deploy OpenSearch cluster (3 nodes). Define dynamic mapping & vector field. Indexer service that pushes docs to OpenSearch. Verify searchability. | Full-text + field-level searchable index. |
| **4 – REST API Layer** | 4 weeks | Design OpenAPI spec. Implement Search API (Gin). File download/upload endpoints (presigned URLs). Pagination, faceting, sorting. | Public JSON API (v1) functional. |
| **5 – MCP Server** | 3 weeks | Define protobuf schema. Implement gRPC server with streaming upload/download. Integration tests with LangChain/Auto-GPT. | MCP endpoint ready for agents. |
| **6 – Security Hardening** | 3 weeks | Deploy Keycloak + OPA. Add JWT validation middleware. TLS everywhere, network policies. Rate-limit plugins. | Authenticated, authorized service. |
| **7 – Scaling & HA** | 4 weeks | Convert dev to multi-node K8s. Set up HPA, PodDisruptionBudgets. Configure OpenSearch replicas, MinIO erasure coding. Add Redis cache. | Production-grade HA cluster. |
| **8 – Observability & Ops** | 2 weeks | Prometheus exporters, Grafana dashboards. Loki/FluentBit log aggregation. Jaeger tracing for API calls. Alerting rules. | Monitoring stack integrated. |
| **9 – Backup / DR** | 2 weeks | MinIO bucket snapshots to another NAS. PostgreSQL WAL archiving. OpenSearch snapshot repository. Disaster-recovery run-books. | Backup procedures documented and tested. |
| **10 – Performance Tuning** | 3 weeks | Load testing (k6/Locust) for ingest & search. Adjust shard count, heap, thread pools. Tune NFS mount options (rsize/wsize). | SLA-compliant system. |
| **11 – Documentation & Release** | 2 weeks | API docs (Swagger UI), MCP spec docs. Deployment Helm charts + values files. User guide for CLI/SDK. Versioned release (v1.0.0). | Production release package. |

**Total Estimated Time:** ~38 weeks (~9 months). Parallel workstreams (e.g., security and observability) can compress schedule.

---

## 11. Summary

The proposed **SISS** architecture leverages battle-tested OSS components (Kubernetes, OpenSearch, MinIO, PostgreSQL, Keycloak) and a clear service decomposition that isolates I/O-heavy ingestion from stateless query handling. It fulfills every core requirement:

- **Rich indexing** of text, XML, JSON with automatic schema inference.
- **Semantic search** via full-text + structured field queries + optional vector similarity.
- **Comprehensive JSON-REST API** plus **MCP gRPC** for AI agents.
- **Horizontal scalability** through sharding, replication, and Kubernetes autoscaling.
- **Self-hosted, open-source** stack with no vendor lock-in.

The design includes concrete security controls, observability, backup/DR, and a phased roadmap that can be delivered incrementally while delivering early value (basic ingest & search) within a few months.
